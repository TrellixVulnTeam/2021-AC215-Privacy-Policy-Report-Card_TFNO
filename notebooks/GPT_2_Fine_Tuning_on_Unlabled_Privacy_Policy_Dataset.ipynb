{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "GPT-2 Fine-Tuning on Unlabled Privacy Policy Dataset",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q0OZWJkq04o"
   },
   "source": [
    "# **Setup Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANRQMIlPvzsi"
   },
   "source": [
    "For this notebook to work, you need to first run the following from the root directory:\n",
    "\n",
    "```shell=\n",
    "python ./Pipeline/Create_Unlabeled_Dataset.py\n",
    "```\n",
    "\n",
    "Doing this will generate `policy_texts.csv` in the root directory. Make sure`file_path` below is the path to this file.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2yMC5354yJTW"
   },
   "source": [
    "file_path = \"./policy_texts.csv\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0XJ_vEWq4Hy"
   },
   "source": [
    "## **Install Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "go9Xw58SVR2K",
    "outputId": "c48557b1-4fea-4393-89e1-5bd813d5fc9f"
   },
   "source": [
    "!pip install transformers"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzpUFWpxVSBi"
   },
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7yz3FQQRVSaS"
   },
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "import shutil\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.utils.layer_utils import count_params\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Transformers\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, GPT2Config"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ5JqxHorBOo"
   },
   "source": [
    "## **Environment Check**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MBJBiJ5YVqDB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5d1ca992-4716-4aed-db48-191b5bd5d3a6"
   },
   "source": [
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
    "# without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(\"tensorflow version\", tf.__version__)\n",
    "print(\"keras version\", tf.keras.__version__)\n",
    "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
    "\n",
    "# Get the number of replicas \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(\"Devices:\", devices)\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/data_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensorflow version 2.6.0\n",
      "keras version 2.6.0\n",
      "Eager Execution Enabled: True\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of replicas: 1\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "All Physical Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bDdKtVkVwch"
   },
   "source": [
    "Run this cell to see what GPU you have. If you get a P100 or T4 GPU that's great. If it's K80, it will still work but it will be slow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZqynk1tVw2t",
    "outputId": "a808bbe4-40c8-4a3e-e042-91244e31487b"
   },
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mon Oct 25 13:46:50 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   71C    P0    70W / 149W |    123MiB / 11441MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yABkL_rBV1u6"
   },
   "source": [
    "## **Utils**\n",
    "\n",
    "Here are some util functions that we will be using for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qOd0mLYzV2Fr"
   },
   "source": [
    "def download_file(packet_url, base_path=\"\", extract=False, headers=None):\n",
    "  if base_path != \"\":\n",
    "    if not os.path.exists(base_path):\n",
    "      os.mkdir(base_path)\n",
    "  packet_file = os.path.basename(packet_url)\n",
    "  with requests.get(packet_url, stream=True, headers=headers) as r:\n",
    "      r.raise_for_status()\n",
    "      with open(os.path.join(base_path,packet_file), 'wb') as f:\n",
    "          for chunk in r.iter_content(chunk_size=8192):\n",
    "              f.write(chunk)\n",
    "  \n",
    "  if extract:\n",
    "    if packet_file.endswith(\".zip\"):\n",
    "      with zipfile.ZipFile(os.path.join(base_path,packet_file)) as zfile:\n",
    "        zfile.extractall(base_path)\n",
    "    else:\n",
    "      packet_name = packet_file.split('.')[0]\n",
    "      with tarfile.open(os.path.join(base_path,packet_file)) as tfile:\n",
    "        tfile.extractall(base_path)\n",
    "\n",
    "class JsonEncoder(json.JSONEncoder):\n",
    "  def default(self, obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, decimal.Decimal):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return super(JsonEncoder, self).default(obj)\n",
    "\n",
    "experiment_name = None\n",
    "def create_experiment():\n",
    "  global experiment_name\n",
    "  experiment_name = \"experiment_\" + str(int(time.time()))\n",
    "\n",
    "  # Create experiment folder\n",
    "  if not os.path.exists(experiment_name):\n",
    "      os.mkdir(experiment_name)\n",
    "\n",
    "def save_data_details(data_details):\n",
    "  with open(os.path.join(experiment_name,\"data_details.json\"), \"w\") as json_file:\n",
    "    json_file.write(json.dumps(data_details,cls=JsonEncoder))\n",
    "\n",
    "def save_model(model,model_name=\"model01\"):\n",
    "\n",
    "  if isinstance(model,TFBertForSequenceClassification):\n",
    "    model.save_weights(os.path.join(experiment_name,model_name+\".h5\"))\n",
    "  else:\n",
    "    # Save the enitire model (structure + weights)\n",
    "    model.save(os.path.join(experiment_name,model_name+\".hdf5\"))\n",
    "\n",
    "    # Save only the weights\n",
    "    model.save_weights(os.path.join(experiment_name,model_name+\".h5\"))\n",
    "\n",
    "    # Save the structure only\n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(experiment_name,model_name+\".json\"), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "def get_model_size(model_name=\"model01\"):\n",
    "  model_size = os.stat(os.path.join(experiment_name,model_name+\".h5\")).st_size\n",
    "  return model_size\n",
    "\n",
    "def evaluate_save_model(model,test_data, training_results,execution_time, learning_rate, batch_size, epochs, optimizer,save=True):\n",
    "    \n",
    "  # Get the model train history\n",
    "  model_train_history = training_results.history\n",
    "  # Get the number of epochs the training was run for\n",
    "  num_epochs = len(model_train_history[\"loss\"])\n",
    "\n",
    "  # Plot training results\n",
    "  fig = plt.figure(figsize=(15,5))\n",
    "  axs = fig.add_subplot(1,2,1)\n",
    "  axs.set_title('Loss')\n",
    "  # Plot all metrics\n",
    "  for metric in [\"loss\",\"val_loss\"]:\n",
    "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
    "  axs.legend()\n",
    "  \n",
    "  axs = fig.add_subplot(1,2,2)\n",
    "  axs.set_title('Accuracy')\n",
    "  # Plot all metrics\n",
    "  for metric in [\"accuracy\",\"val_accuracy\"]:\n",
    "      axs.plot(np.arange(0, num_epochs), model_train_history[metric], label=metric)\n",
    "  axs.legend()\n",
    "\n",
    "  plt.show()\n",
    "  \n",
    "  # Evaluate on test data\n",
    "  evaluation_results = model.evaluate(test_data)\n",
    "  print(evaluation_results)\n",
    "  \n",
    "  if save:\n",
    "    # Save model\n",
    "    save_model(model, model_name=model.name)\n",
    "    model_size = get_model_size(model_name=model.name)\n",
    "\n",
    "    # Save model history\n",
    "    with open(os.path.join(experiment_name,model.name+\"_train_history.json\"), \"w\") as json_file:\n",
    "        json_file.write(json.dumps(model_train_history,cls=JsonEncoder))\n",
    "\n",
    "    trainable_parameters = count_params(model.trainable_weights)\n",
    "    non_trainable_parameters = count_params(model.non_trainable_weights)\n",
    "\n",
    "    # Save model metrics\n",
    "    metrics ={\n",
    "        \"trainable_parameters\":trainable_parameters,\n",
    "        \"execution_time\":execution_time,\n",
    "        \"loss\":evaluation_results[0],\n",
    "        \"accuracy\":evaluation_results[1],\n",
    "        \"model_size\":model_size,\n",
    "        \"learning_rate\":learning_rate,\n",
    "        \"batch_size\":batch_size,\n",
    "        \"epochs\":epochs,\n",
    "        \"name\": model.name,\n",
    "        \"optimizer\":type(optimizer).__name__\n",
    "    }\n",
    "    with open(os.path.join(experiment_name,model.name+\"_model_metrics.json\"), \"w\") as json_file:\n",
    "        json_file.write(json.dumps(metrics,cls=JsonEncoder))"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ypbW8rkrIuY"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uKmjX-opSNu"
   },
   "source": [
    "# **Prepare Training Data** \n",
    "\n",
    "We will be working with privacy policy data collected from the internet. We will explore the dataset, prepare the data for finetuning GPT2.\n",
    "\n",
    "**The Task:** Finetune GPT2 to build a language model on privacy policy text. We aim to train a model that will generate text that resembles the type of writing you see in a privacy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aPKwZKgtctp"
   },
   "source": [
    "## **Load Data**\n",
    "\n",
    "* Read-in data as lists."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4AGC-LSkF2l",
    "outputId": "6afedb02-8a44-49a2-ec41-99f7dd408a40"
   },
   "source": [
    "df_policies = pd.read_csv(file_path , index_col=0)\n",
    "df_policies.dropna(inplace=True)\n",
    "training_data = list(df_policies.paragraph_text)\n",
    "len(training_data)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2675"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-JJUy7ltiX7"
   },
   "source": [
    "## **View Text**\n",
    "\n",
    "Let's take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbvwOPwUtitb",
    "outputId": "30057005-2050-41e7-8619-cf6c295415b0"
   },
   "source": [
    "# Generate a random sample of index\n",
    "data_samples = np.random.randint(0,high=len(training_data)-1, size=10)\n",
    "for i,data_idx in enumerate(data_samples):\n",
    "  print(\"Text:\",training_data[data_idx])"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text: 수신하지 않을 마케팅 또는 프로모션 이메일이 수신되는 경우, 각 메시지에 포함된 “수신 거부” 안내를 따르십시오.\n",
      "Text: Open your Google Settings app > Ads >Enable “Opt Out of Interest-Based Advertising” or “Opt Out of Ads Personalization”.\n",
      "Text: 귀하는 개인 연락처 정보를 전혀 공유하지 않고도 많은 Mattel 서비스를 이용할 수 있습니다. 귀하의 선택에 따라 귀하가 개인 연락처 정보, 개인 정보, 로그인 정보, 관심사 또는 인구 통계적 정보, 또는 귀하나 귀하의 자녀에 관한 설문조사 정보를 우리와 공유할 수 있는 경우는 아래와 같습니다:\n",
      "Text: Information that we collect are \"NETWORK STATUS INFORMATION\", \"WIFI STATUS INFORMATION\", \" INTERNAL DATA STORAGE\"In order to show the advertisement, we need requires Internet Connection checking, either via non-Wi-Fi or Wi-Fi.Some of our apps may need External Data Storage, it is used only to improve the user experience in our apps such as storing the results of user exercises.Information Security\n",
      "Text: Collecting User Information\n",
      "Text: Samuel J or Eznetsoft uses remarketing with Google AdWords and analytics to display content-specific advertisements to visitors that have previously visited our site when those visitors go to other websites that have the Google Display Network implemented.\n",
      "Text: 8. DISCLAIMER OF WARRANTIES\n",
      "Text: \n",
      "FamilySearch Terms of Use (Updated 2021-09-27)  | Privacy Notice (Updated 2021-04-06)\n",
      "\n",
      "Text: 위치 정보.\n",
      "Text: 고급형 Mattel 서비스\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRFZyv7mto2R"
   },
   "source": [
    "## **Tokenize Data for GPT2**\n",
    "\n",
    "We will use the `distilgpt2` version of pre trained GPT2 model to tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PD4xml28tpUN",
    "outputId": "65fc94e8-fbb2-4bf3-f284-eb31a355369d"
   },
   "source": [
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Tokenize data\n",
    "training_data_tokenized = []\n",
    "for data in training_data:\n",
    "\n",
    "  tokenized_text = tokenizer.encode(data)\n",
    "  training_data_tokenized.append(tokenized_text)\n",
    "\n",
    "print(len(training_data_tokenized))\n",
    "print(len(training_data_tokenized[0]),training_data_tokenized[0][:20])"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2550 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2675\n",
      "18 [48948, 7820, 1849, 1222, 35118, 286, 5765, 1849, 7, 5956, 6153, 2362, 1987, 400, 11, 33448, 8, 29064]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlvFhxybttkD"
   },
   "source": [
    "## **Generate Training Data**\n",
    "\n",
    "For the training we need inputs and lables but we only have privacy policy texts. In lecture we learnt that language models are trained in a semi supervised way where we generate inputs and labels from the input text. \n",
    "\n",
    "<br>\n",
    "\n",
    "To generate inputs and lables for training we will chunk the input text into blocks of size `100`. Then our labels will be the same as inputs but one position shifted to the right."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RFegEFVtvxT",
    "outputId": "018ddaf6-674b-45f3-bdc5-5ce46249514e"
   },
   "source": [
    "# Split into blocks\n",
    "training_chunks = []\n",
    "block_size = 100\n",
    "for tokenized_text in training_data_tokenized:\n",
    "  for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
    "      training_chunks.append(tokenized_text[i:i + block_size])\n",
    "\n",
    "# Generate inputs and labels\n",
    "inputs = []\n",
    "labels = []\n",
    "for ex in training_chunks:\n",
    "    inputs.append(ex[:-1])\n",
    "    labels.append(ex[1:])\n",
    "\n",
    "print(\"inputs length:\",len(inputs))\n",
    "print(\"labels length:\",len(labels))"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs length: 1591\n",
      "labels length: 1591\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Toe8GkoXuSvt",
    "outputId": "a0b237bb-aa94-4e60-ccce-4066980234ff"
   },
   "source": [
    "print(\"input:\",len(inputs[0]),inputs[0][:20])\n",
    "print(\"labels:\",len(labels[0]),labels[0][:20])"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input: 99 [32, 366, 44453, 1, 318, 281, 5002, 286, 1366, 326, 257, 5313, 14413, 460, 3758, 284, 534, 6444, 11, 543]\n",
      "labels: 99 [366, 44453, 1, 318, 281, 5002, 286, 1366, 326, 257, 5313, 14413, 460, 3758, 284, 534, 6444, 11, 543, 743]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUX6TGCxub66"
   },
   "source": [
    "## **Create TF Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nDYekpVucHW",
    "outputId": "8bc19eb8-9f39-4f4f-96a7-b3656b1fa745"
   },
   "source": [
    "BATCH_SIZE = 12\n",
    "TRAIN_SHUFFLE_BUFFER_SIZE = len(inputs)\n",
    "\n",
    "# Create TF Dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "\n",
    "#############\n",
    "# Train data\n",
    "#############\n",
    "train_data = train_data.shuffle(buffer_size=TRAIN_SHUFFLE_BUFFER_SIZE)\n",
    "train_data = train_data.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_data = train_data.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"train_data\",train_data)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_data <PrefetchDataset shapes: ((12, 99), (12, 99)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYt9skk1utzX"
   },
   "source": [
    "# **Finetune GPT2 Pretrained Model on Privacy Policy Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBRpVTsWrw7V"
   },
   "source": [
    "## **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvUKIQdZuuem",
    "outputId": "5d159911-1730-4526-fae0-34b450a9cfd5"
   },
   "source": [
    "############################\n",
    "# Training Params\n",
    "############################\n",
    "learning_rate = 3e-5 \n",
    "epsilon=1e-08\n",
    "clipnorm=1.0\n",
    "epochs = 5 # 100\n",
    "\n",
    "# Free up memory\n",
    "K.clear_session()\n",
    "\n",
    "# Build the model\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "# Optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=clipnorm)\n",
    "# Loss\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "# Compile\n",
    "model.compile(loss=[loss, *[None] * model.config.n_layer],\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[metric])\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "training_results = model.fit(\n",
    "        train_data.take(100), # train_data.take(100) for testing\n",
    "        epochs=epochs, \n",
    "        verbose=1)\n",
    "execution_time = (time.time() - start_time)/60.0\n",
    "print(\"Training execution time (mins)\",execution_time)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"tfgp_t2lm_head_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "transformer (TFGPT2MainLayer multiple                  81912576  \n",
      "=================================================================\n",
      "Total params: 81,912,576\n",
      "Trainable params: 81,912,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "100/100 [==============================] - 61s 488ms/step - loss: 2.5432 - logits_loss: 2.5432 - logits_accuracy: 0.3873 - past_key_values_1_accuracy: 5.5661e-04 - past_key_values_2_accuracy: 5.6292e-04 - past_key_values_3_accuracy: 9.8134e-04 - past_key_values_4_accuracy: 8.3719e-04 - past_key_values_5_accuracy: 3.6476e-04 - past_key_values_6_accuracy: 6.5516e-04\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 2.1648 - logits_loss: 2.1648 - logits_accuracy: 0.4610 - past_key_values_1_accuracy: 5.2609e-04 - past_key_values_2_accuracy: 5.3416e-04 - past_key_values_3_accuracy: 0.0011 - past_key_values_4_accuracy: 7.5021e-04 - past_key_values_5_accuracy: 3.4407e-04 - past_key_values_6_accuracy: 6.4639e-04\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 1.9574 - logits_loss: 1.9574 - logits_accuracy: 0.5024 - past_key_values_1_accuracy: 5.8361e-04 - past_key_values_2_accuracy: 5.4924e-04 - past_key_values_3_accuracy: 0.0011 - past_key_values_4_accuracy: 7.5442e-04 - past_key_values_5_accuracy: 3.3389e-04 - past_key_values_6_accuracy: 5.7590e-04\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 1.8065 - logits_loss: 1.8065 - logits_accuracy: 0.5336 - past_key_values_1_accuracy: 5.6257e-04 - past_key_values_2_accuracy: 4.6787e-04 - past_key_values_3_accuracy: 9.2558e-04 - past_key_values_4_accuracy: 7.2145e-04 - past_key_values_5_accuracy: 3.5809e-04 - past_key_values_6_accuracy: 5.1733e-04\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 1.6811 - logits_loss: 1.6811 - logits_accuracy: 0.5590 - past_key_values_1_accuracy: 6.0571e-04 - past_key_values_2_accuracy: 5.1171e-04 - past_key_values_3_accuracy: 0.0011 - past_key_values_4_accuracy: 7.6178e-04 - past_key_values_5_accuracy: 3.4336e-04 - past_key_values_6_accuracy: 5.3556e-04\n",
      "Training execution time (mins) 5.364365092913309\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V4qixolrzuu"
   },
   "source": [
    "## **Model Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8Zj74t7UWBG",
    "outputId": "ec30104b-c29a-40ac-cddf-6fc0832fc7fd"
   },
   "source": [
    "model.config"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"distilgpt2\",\n",
       "  \"_num_labels\": 1,\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0\n",
       "  },\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 6,\n",
       "  \"n_positions\": 1024,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inzBrvb5oQVN"
   },
   "source": [
    "# **Is Our Model Really Fine-Tuned?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUj5fdhSpkWe"
   },
   "source": [
    "#### **Generate Text from Privacy-Policy-Trained Model**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "xExp6C8Su7Fd",
    "outputId": "cd6efd1d-b923-4c89-bdc7-57c48e227787"
   },
   "source": [
    "# Input text\n",
    "input_text = \"Your location data\"\n",
    "\n",
    "# Tokenize Input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "print(\"input_ids\",input_ids)\n",
    "\n",
    "# Generate outout\n",
    "outputs = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=75, \n",
    "    top_p=0.80, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "display(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids tf.Tensor([[7120 4067 1366]], shape=(1, 3), dtype=int32)\n",
      "Generated text:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your location data will be used to set the time of your request. \\nWhat happens when you send an email to a Facebook Group when the service has been activated, or when your account has been activated. \\nYou may leave an account with your password, so that the information that you request is more freely available to you. \\nYou may set the time of'"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFCWvmIyumNw"
   },
   "source": [
    "**GENERATED TEXT FROM <font color=\"green\">PRIVACY POLICY </font>FINE-TUNED MODEL**\n",
    "\n",
    "\"Your location data will be used to set the time of your request. \\nWhat happens when you send an email to a Facebook Group when the service has been activated, or when your account has been activated. \\nYou may leave an account with your password, so that the information that you request is more freely available to you. \\nYou may set the time of...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nxklFZ-u6I-"
   },
   "source": [
    "#### **Generate Text from Model Pre-Trained on COVID News Articles**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8_eloZo3Ya0",
    "outputId": "af40a6ce-958c-4953-b688-4e8bd06a44c5"
   },
   "source": [
    "# Dowload trained model on 100 epochs and full dataset (takes around 3 hours to train)\n",
    "start_time = time.time()\n",
    "download_file(\"https://github.com/dlops-io/models/releases/download/v1.0/distilgpt2_covid.zip\", base_path=\"models\", extract=True)\n",
    "execution_time = (time.time() - start_time)/60.0\n",
    "print(\"Download execution time (mins)\",execution_time)"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Download execution time (mins) 0.1273188312848409\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuZ01O-03uxy",
    "outputId": "87eeed1d-11fc-4e86-bf16-3247ce32b78d"
   },
   "source": [
    "# Load the previously trained model\n",
    "loaded_model = TFGPT2LMHeadModel.from_pretrained('./models/distilgpt2_covid/')"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./models/distilgpt2_covid/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "ORxlhR1voHo5",
    "outputId": "6d56687e-6eeb-4f01-ca9d-a4a9d6855088"
   },
   "source": [
    "# Input text\n",
    "input_text = \"Your location data\"\n",
    "\n",
    "# Tokenize Input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "print(\"input_ids\",input_ids)\n",
    "\n",
    "# Generate outout\n",
    "outputs = loaded_model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=75, \n",
    "    top_p=0.80, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "display(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids tf.Tensor([[7120 4067 1366]], shape=(1, 3), dtype=int32)\n",
      "Generated text:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your location data can save lives,\" Gerow says. \"If there is not a change in the plan, then the only way to avoid it is to have people log in and make a schedule.\" You can also make a schedule using home data, such as your birthday and your school\\'s total.  You can also make the decision to save money or move forward without having'"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpjWMypeu8B3"
   },
   "source": [
    "**GENERATED TEXT FROM <font color=\"red\">COVID</font> FINE-TUNED MODEL**\n",
    "\n",
    "\"Your location data can save lives,\" Gerow says. \"If there is not a change in the plan, then the only way to avoid it is to have people log in and make a schedule.\" You can also make a schedule using home data, such as your birthday and your school\\'s total.  You can also make the decision to save money or move forward without having...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQUfvb46ps_r"
   },
   "source": [
    "# Notice how the Covid-trained model generates text following the initial \"Your location data\" with Covid-related sentences. This is different from the model we trained which generates text that resembles a privacy policy. "
   ]
  }
 ]
}