{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Privacy Policy Report Card Milestone 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMXL4o7ziMVC"
      },
      "source": [
        "# **Privacy Policy Report Card**\n",
        "\n",
        "## **Milestone 2 Notebook**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCP2mNi-itmv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Problem Definition and Proposed Solution\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMjQhi1-jVjG"
      },
      "source": [
        "We are used to consenting to a website’s or app’s privacy policy assuming that our personal data will be used responsibly. This is not always the case. Most privacy policies are either too long to read or too difficult to understand. This means that most of us will have no choice but to continue accepting terms that we don’t fully understand.\n",
        "\n",
        "We propose training a model using NLP to recognize data usage and collection clauses within online privacy policies, and to present a user with a “report card” outlining what data the service collects about the user, and how that data is used. This should allow users to feel more confident in their decisions regarding online privacy. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHmU7i8kkOEb"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#The Data\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ToLnoDZkUmM"
      },
      "source": [
        "###Labeled Training Data\n",
        "\n",
        "For our training dataset, we chose the APP 350 Corpus a collection of 350 privacy policies whose contents have been annotated, paragraph by paragraph, by legal experts. The APP 350 used 60 annotations to code privacy policy contents. While the APP 350 is quite a clean dataset on its own, it will still require some work to get it into a structure that can be used for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUGiD75wyHxf",
        "outputId": "e0433194-549e-4f69-bf1a-80594bccaf4b"
      },
      "source": [
        "!unzip Data/APP-350_v1.1.zip\n",
        "!unzip Data/MAPS_Policies_Dataset_v1.0.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Data/APP-350_v1.1.zip\n",
            "replace APP-350_v1.1/documentation/annotator_agreement.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  Data/MAPS_Policies_Dataset_v1.0.zip\n",
            "replace MAPS Policies Dataset/LICENSE? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvegQEgpiLuI"
      },
      "source": [
        "import pandas as pd\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "######\n",
        "#Reading the data\n",
        "######\n",
        "\n",
        "\n",
        "directory = os.path.normpath('APP-350_v1.1/annotations')\n",
        "\n",
        "policy_list = os.listdir(directory)\n",
        "\n",
        "raw_data = []\n",
        "\n",
        "for file in policy_list:\n",
        "    path = os.path.join(directory, file)\n",
        "    with open (path) as f:\n",
        "        record = yaml.safe_load(f)\n",
        "\n",
        "    policy_id = record['policy_id']\n",
        "    policy_name = record ['policy_name']\n",
        "    contains_synthetic = record['contains_synthetic']\n",
        "\n",
        "    for segment in record['segments']:\n",
        "        segment.update({\n",
        "            'policy_id': policy_id,\n",
        "            'policy_name': policy_name,\n",
        "            'contains_synthetic': contains_synthetic, })\n",
        "        raw_data.append(segment)\n",
        "\n",
        "\n",
        "with open(os.path.normpath('APP-350_v1.1/features.yml'))as f:\n",
        "    features = yaml.safe_load(f)\n",
        "\n",
        "tags = []\n",
        "for i in features['data_types']:\n",
        "    for p in i['practices']:\n",
        "        tags.append(p)\n",
        "\n",
        "\n",
        "APP_350 = pd.DataFrame(raw_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4Fi6pdSmdZi"
      },
      "source": [
        "Because the APP 350 uses so many annotations, the individual occurrences of each is too small to meaningfully train a model. However, these 60 annotations fall into 6 broad categories:\n",
        "\n",
        "1.   Web Identifier and Trackers\n",
        "2.   Demographic Info\n",
        "3.   Contact Info\n",
        "4.   Location Data\n",
        "5.   Single Sign On\n",
        "6.   Sharing with 3rd parties\n",
        "\n",
        "Aggregating annotations into these catagories allows the model to recognize general patterns more easily, and makes the final product more easily understandable for the user. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-hE4od2max7"
      },
      "source": [
        "####\n",
        "#Making the labels\n",
        "####\n",
        "\n",
        "def parse_annotations(annotation, tag):\n",
        "    \"\"\"\n",
        "    Function for parsing APP_350 annotations into binary response\n",
        "    :param annotation: List of dicts containing 'practice' and 'modality' annotations\n",
        "    :param tag: str. the tag being searched for\n",
        "    :return: bool - does the annotation contain the given tag\n",
        "    \"\"\"\n",
        "    practice_performed = False\n",
        "    for n in annotation:\n",
        "        if n['practice'] == tag and n['modality'] == 'PERFORMED':\n",
        "            practice_performed = True\n",
        "\n",
        "    return practice_performed\n",
        "\n",
        "\n",
        "\n",
        "for tag in tags:\n",
        "    col_name = 'y_' + tag\n",
        "    APP_350[col_name] = APP_350['annotations'].apply(parse_annotations, args=[tag])\n",
        "\n",
        "\n",
        "categories = ['3RD',\n",
        "              'LOCATION',\n",
        "              'DEMOGRAPHIC',\n",
        "              'CONTACT',\n",
        "              'IDENTIFIER',\n",
        "              'SSO',\n",
        "              ]\n",
        "\n",
        "targets = [i for i in APP_350.columns if 'y_' in i]\n",
        "\n",
        "for cat in categories:\n",
        "    cols = [i for i in targets if cat in i.upper()]\n",
        "    APP_350[cat] = APP_350[cols].any(axis = 1, bool_only=True)\n",
        "\n",
        "rel_cols = ['policy_id','policy_name','segment_id', 'segment_text', *categories]\n",
        "\n",
        "cleaned_data = APP_350[rel_cols]\n",
        "\n",
        "save_path = os.path.normpath('Data/Labeled_Data.csv')\n",
        "if not os.path.exists(save_path):\n",
        "    cleaned_data.to_csv(save_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0SITHwjoYhr"
      },
      "source": [
        "###Unlabeled Tuning Data\n",
        "\n",
        "The limited availability of labeled training data makes it necessary to scrape unlabeled privacy policies from the web to train larger models. This data must be parsed and divided into paragraphs to make it resemble the training data that will be used for the final classification. \n",
        "\n",
        "For this demonstration, we have limited the number of URLs scraped, however the base dataset includes 400,000 unlabeled privacy policies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vZGvj3Epr7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3358b716-c1fe-4ffd-d190-86420c019679"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "1. take a url as input and make it into a bs4 soup object\n",
        "\"\"\"\n",
        "from urllib.request import urlopen\n",
        "\n",
        "def make_soup(url):\n",
        "    # print('in make_soup')\n",
        "    html = urlopen(url).read()\n",
        "    # print(html)\n",
        "    return BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "\"\"\"\n",
        "2. take a soup object and find all the links on the page\n",
        "\"\"\"\n",
        "def find_links(soup):\n",
        "    links = []\n",
        "    for link in soup.find_all('a'):\n",
        "        links.append(link.get('href'))\n",
        "    return links\n",
        "\n",
        "\"\"\"\n",
        "3. take a soup object and find all the text on the page\n",
        "\"\"\"\n",
        "def find_text(soup):\n",
        "    text = soup.get_text()\n",
        "    return text\n",
        "\n",
        "\"\"\"\n",
        "4. take a soup object and find all the images on the page\n",
        "\"\"\"\n",
        "def find_images(soup):\n",
        "    images = []\n",
        "    for image in soup.find_all('img'):\n",
        "        images.append(image.get('src'))\n",
        "    return images\n",
        "\n",
        "\"\"\"\n",
        "5. take a soup object and find all the tables on the page\n",
        "\"\"\"\n",
        "def find_tables(soup):\n",
        "    tables = []\n",
        "    for table in soup.find_all('table'):\n",
        "        tables.append(table)\n",
        "    return tables\n",
        "\n",
        "\"\"\"\n",
        "6. take a soup object and find all the forms on the page\n",
        "\"\"\"\n",
        "def find_forms(soup):\n",
        "    forms = []\n",
        "    for form in soup.find_all('form'):\n",
        "        forms.append(form)\n",
        "    return forms\n",
        "\n",
        "\"\"\"\n",
        "7. take a soup object and find all the headings on the page\n",
        "\"\"\"\n",
        "def find_headings(soup):\n",
        "    headings = []\n",
        "    for heading in soup.find_all(re.compile('^h[1-6]$')):\n",
        "        headings.append(heading)\n",
        "    return headings\n",
        "\n",
        "\"\"\"\n",
        "8. take a soup object and find all the paragraphs on the page\n",
        "\"\"\"\n",
        "def find_paragraphs(soup):\n",
        "    paragraphs = []\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        paragraphs.append(paragraph)\n",
        "    return paragraphs\n",
        "\n",
        "\"\"\"\n",
        "9. take a soup object and find all the divs on the page\n",
        "\"\"\"\n",
        "def find_divs(soup):\n",
        "    divs = []\n",
        "    for div in soup.find_all('div'):\n",
        "        divs.append(div)\n",
        "    return divs\n",
        "\n",
        "\"\"\"\n",
        "10. take a soup object and find all the spans on the page\n",
        "\"\"\"\n",
        "def find_spans(soup):\n",
        "    spans = []\n",
        "    for span in soup.find_all('span'):\n",
        "        spans.append(span)\n",
        "    return spans\n",
        "\n",
        "\"\"\"\n",
        "11. take a soup object and find all the list items on the page\n",
        "\"\"\"\n",
        "def find_list_items(soup):\n",
        "    list_items = []\n",
        "    for list_item in soup.find_all('li'):\n",
        "        list_items.append(list_item)\n",
        "    return list_items\n",
        "\n",
        "\"\"\"\n",
        "12. take a soup object and find all the unordered list on the page\n",
        "\"\"\"\n",
        "def find_unordered_lists(soup):\n",
        "    unordered_lists = []\n",
        "    for unordered_list in soup.find_all('ul'):\n",
        "        unordered_lists.append(unordered_list)\n",
        "    return unordered_lists\n",
        "\n",
        "\"\"\"\n",
        "13. take a soup object and find all the ordered list on the page\n",
        "\"\"\"\n",
        "def find_ordered_lists(soup):\n",
        "    ordered_lists = []\n",
        "    for ordered_list in soup.find_all('ol'):\n",
        "        ordered_lists.append(ordered_list)\n",
        "    return ordered_lists\n",
        "\n",
        "\"\"\"\n",
        "14. take a soup object and find all the h1-h6 on the page\n",
        "\"\"\"\n",
        "import re\n",
        "def find_headings(soup):\n",
        "    headings = []\n",
        "    for heading in soup.find_all(re.compile('^h[1-6]$')):\n",
        "        headings.append(heading)\n",
        "    return headings\n",
        "\n",
        "\"\"\"\n",
        "15. take a soup object and find all the paragraphs on the page\n",
        "\"\"\"\n",
        "def find_paragraphs(soup):\n",
        "    paragraphs = []\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        paragraphs.append(paragraph)\n",
        "    return paragraphs\n",
        "\n",
        "\"\"\"\n",
        "16. Given a list of URLs, extract the paragraphs and return a pandas DataFrame\n",
        "\"\"\"\n",
        "def create_df(urls):\n",
        "    print(\"in create_df\")\n",
        "    df = pd.DataFrame()\n",
        "    for i, url in enumerate(urls):\n",
        "        try:\n",
        "            soup = make_soup(url)\n",
        "            paragraphs = find_paragraphs(soup)\n",
        "            paragraphs_list = [p.text for p in paragraphs]\n",
        "            tuples_list = [(url, i, p) for i,p in enumerate(paragraphs_list)]\n",
        "            for tup in tuples_list:\n",
        "                df=df.append(pd.Series(tup, index= ['url','paragraph_index', 'paragraph_text']), ignore_index=True)\n",
        "\n",
        "        except:\n",
        "            print('error at index', i, url)\n",
        "            pass\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "path_to_policy_urls_csv = os.path.normpath('MAPS Policies Dataset/april_2018_policies.csv')\n",
        "num_policies_to_extract = 100\n",
        "# load the privacy policies URL csv file\n",
        "df_policies = pd.read_csv(path_to_policy_urls_csv)\n",
        "# print(df_policies.head(5))\n",
        "\n",
        "# select 10 urls for demo\n",
        "df_policies = df_policies.iloc[:num_policies_to_extract]\n",
        "\n",
        "# extract urls of privacy policies\n",
        "# TODO: Needs to be modified to include all urls in the dataset. Currently just looks at the first \"Final URL\" of each row.\n",
        "urls = [item.split(\"'Final URL': '\")[1].split(\"'}\")[0] for item in df_policies['Policy Sources']]\n",
        "# print(urls)\n",
        "# Create a dataframe with the policy texts\n",
        "df = create_df(urls)\n",
        "print(df.head())\n",
        "print(f\"DataFrame consists of {df.shape[0]} paragraphs from {num_policies_to_extract} policies.\")\n",
        "\n",
        "# Save dataframe as csv file\n",
        "df.to_csv('policy_texts.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in create_df\n",
            "error at index 3 http://www.kidzooly.com/privacy.html\n",
            "error at index 22 http://www.kidzooly.com/privacy.html\n",
            "error at index 25 http://www.kidloland.com/privacypolicy.php\n",
            "error at index 26 http://www.kidzooly.com/privacy.html#privacy\n",
            "error at index 28 http://corporate.mattel.com/privacy-statement-shared.aspx\n",
            "error at index 29 http://corporate.mattel.com/privacy-statement-shared.aspx\n",
            "error at index 32 http://corporate.mattel.com/privacy-statement-shared.aspx\n",
            "error at index 33 http://corporate.mattel.com/privacy-statement-shared.aspx\n",
            "error at index 36 http://corporate.mattel.com/privacy-statement-shared.aspx\n",
            "error at index 37 http://ellenwhite.org/content/article/egw-writings-privacy-policy\n",
            "error at index 38 http://ellenwhite.org/content/article/egw-writings-privacy-policy\n",
            "error at index 40 http://ellenwhite.org/content/article/egw-writings-privacy-policy?numFound=2&collection=true&query=privacy+policy&curr=0&sqid=591334551\n",
            "error at index 41 https://www.bible.com/privacy\n",
            "error at index 61 http://bu.com.vn/privacy-policy/\n",
            "error at index 62 http://zonao.com/privacypolicy.html\n",
            "error at index 68 http://ww38.gze.es/privacy.html\n",
            "error at index 69 http://www.parents2parentsapps.com/privacy.html\n",
            "error at index 76 http://www.touchzing.com/privacy/\n",
            "error at index 96 http://bíblia.org/\n",
            "   paragraph_index  ...                                                url\n",
            "0              0.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
            "1              1.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
            "2              2.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
            "3              3.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
            "4              4.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "DataFrame consists of 2629 paragraphs from 100 policies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-tIAyobsN6B"
      },
      "source": [
        "###EDA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9Tmqr2tXh7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "588d7829-e017-4fbf-8e83-09168e38a298"
      },
      "source": [
        "######\n",
        "#Labeled Data\n",
        "#######\n",
        "\n",
        "cleaned_data.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>policy_id</th>\n",
              "      <th>policy_name</th>\n",
              "      <th>segment_id</th>\n",
              "      <th>segment_text</th>\n",
              "      <th>3RD</th>\n",
              "      <th>LOCATION</th>\n",
              "      <th>DEMOGRAPHIC</th>\n",
              "      <th>CONTACT</th>\n",
              "      <th>IDENTIFIER</th>\n",
              "      <th>SSO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4305</th>\n",
              "      <td>243</td>\n",
              "      <td>io.utk.android</td>\n",
              "      <td>2</td>\n",
              "      <td>Passwords The UTK.io staff will NEVER ask for ...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4739</th>\n",
              "      <td>17</td>\n",
              "      <td>com.atomicadd.fotos</td>\n",
              "      <td>11</td>\n",
              "      <td>Location information When you use AtomicAdd se...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7824</th>\n",
              "      <td>98</td>\n",
              "      <td>Xender</td>\n",
              "      <td>7</td>\n",
              "      <td>(d) We may collect and use such data for promo...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6871</th>\n",
              "      <td>336</td>\n",
              "      <td>Viber</td>\n",
              "      <td>39</td>\n",
              "      <td>Here are a few additional important things you...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12536</th>\n",
              "      <td>164</td>\n",
              "      <td>com.eharmony</td>\n",
              "      <td>10</td>\n",
              "      <td>Purchase Information. To process purchases, we...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       policy_id          policy_name  segment_id  ... CONTACT  IDENTIFIER    SSO\n",
              "4305         243       io.utk.android           2  ...   False       False  False\n",
              "4739          17  com.atomicadd.fotos          11  ...   False        True  False\n",
              "7824          98               Xender           7  ...   False       False  False\n",
              "6871         336                Viber          39  ...   False        True  False\n",
              "12536        164         com.eharmony          10  ...    True       False  False\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ffJ_hBOtiBO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "697709aa-bccd-4031-c3e2-bf22c4447a4c"
      },
      "source": [
        "cleaned_data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>policy_id</th>\n",
              "      <th>segment_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>15507.000000</td>\n",
              "      <td>15507.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>174.877862</td>\n",
              "      <td>48.647127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>102.593012</td>\n",
              "      <td>75.952880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>82.000000</td>\n",
              "      <td>11.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>171.000000</td>\n",
              "      <td>26.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>263.000000</td>\n",
              "      <td>52.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>350.000000</td>\n",
              "      <td>607.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          policy_id    segment_id\n",
              "count  15507.000000  15507.000000\n",
              "mean     174.877862     48.647127\n",
              "std      102.593012     75.952880\n",
              "min        1.000000      0.000000\n",
              "25%       82.000000     11.000000\n",
              "50%      171.000000     26.000000\n",
              "75%      263.000000     52.000000\n",
              "max      350.000000    607.000000"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA0xCaSutmV7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c0c6e145-cfcd-4cf9-a3d0-22de9791918e"
      },
      "source": [
        "######\n",
        "#Unlabeled Data\n",
        "#######\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paragraph_index</th>\n",
              "      <th>paragraph_text</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>Privacy Policy  &amp; Term of Use (Last updated Se...</td>\n",
              "      <td>https://www.eznetsoft.com/index.php/about-us/p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Samuel J or Eznetsoft is committed to protecti...</td>\n",
              "      <td>https://www.eznetsoft.com/index.php/about-us/p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>In general, you can visit us on the Web withou...</td>\n",
              "      <td>https://www.eznetsoft.com/index.php/about-us/p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>Samuel J or Eznetsoft will not sell, rent or d...</td>\n",
              "      <td>https://www.eznetsoft.com/index.php/about-us/p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>Information that we gather and track, in accor...</td>\n",
              "      <td>https://www.eznetsoft.com/index.php/about-us/p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   paragraph_index  ...                                                url\n",
              "0              0.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
              "1              1.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
              "2              2.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
              "3              3.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
              "4              4.0  ...  https://www.eznetsoft.com/index.php/about-us/p...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktWEWvmQx2aY"
      },
      "source": [
        "###Prepping the data for modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJb_yjUcx5Z3",
        "outputId": "014d5e6d-4a0e-4a79-def0-00294eb6e464"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import string\n",
        "\n",
        "data = cleaned_data\n",
        "x = data.segment_text\n",
        "y = data[[\n",
        "    'IDENTIFIER',\n",
        "    '3RD',\n",
        "    'LOCATION',\n",
        "    'DEMOGRAPHIC',\n",
        "    'CONTACT',\n",
        "    'SSO']]\n",
        "\n",
        "\n",
        "y = y.astype(int)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y)\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "TRAIN_SHUFFLE_BUFFER_SIZE = len(x_train)\n",
        "VALIDATION_SHUFFLE_BUFFER_SIZE = len(x_test)\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def standardize_text(input_string):\n",
        "  output_string = tf.strings.lower(input_string)\n",
        "  output_string = tf.strings.regex_replace(output_string, \"<br />\", \" \")\n",
        "  output_string = tf.strings.regex_replace(output_string, \"[%s]\" % re.escape(string.punctuation), \"\")\n",
        "  return output_string\n",
        "\n",
        "text_vectorizer = keras.layers.TextVectorization(\n",
        "    standardize=standardize_text,\n",
        "    max_tokens=25000,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=1500,\n",
        ")\n",
        "\n",
        "text_data = tf.data.Dataset.from_tensor_slices(x.values)\n",
        "text_vectorizer.adapt(text_data.batch(64))\n",
        "\n",
        "x_train = text_vectorizer.apply(x_train)\n",
        "x_test = text_vectorizer.apply(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:2215: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muvOmOuqt0Wf"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Models\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqteQ4FAwNBI"
      },
      "source": [
        "Our project's central task is text classification. However, it does have one unique challenge; because our labels are independent, any given text can belong to multiple “classes”. For example, a given paragraph may contain clauses that allow for the collection of both location data and demographic information. \n",
        "\n",
        "To solve this challenge, we intend to implement an ensemble model consisting of binary response outputs to allow for this kind of multi-classification. Our current approach is to train separate binary-response models, which we are evaluating independently. \n",
        "\n",
        "Because of the computational expense of building and training SOTA NLP models, we have chosen to prototype using fairly simple, home-grown models for iteration and testing. The best method we have found for training these is demonstrated below. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiMMxzywxylo",
        "outputId": "5cdb6fc8-203d-4931-81b2-a3a4c68a23ae"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "y_label = \"IDENTIFIER\"\n",
        "\n",
        "\n",
        "y_trainM = y_train[y_label]\n",
        "y_testM  = y_test[y_label]\n",
        "\n",
        "\n",
        "####\n",
        "#Prototype/Proof of Concept Model\n",
        "####\n",
        "\n",
        "def binary_cnn_with_embeddings(sequence_length, vocab_size, embedding_dim,\n",
        "                              model_name='cnn_with_embeddings'):\n",
        "    model_input = keras.layers.Input(shape=(sequence_length))\n",
        "\n",
        "    hidden = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding\")(model_input)\n",
        "\n",
        "    hidden = keras.layers.Conv1D(filters=256, kernel_size=5, padding=\"valid\", activation=\"relu\", strides=3)(hidden)\n",
        "    hidden = keras.layers.GlobalMaxPooling1D()(hidden)\n",
        "\n",
        "    hidden = keras.layers.Dense(units=128, activation=\"tanh\")(hidden)\n",
        "    hidden = keras.layers.Dense(units=64, activation=\"tanh\")(hidden)\n",
        "    hidden = keras.layers.Dense(units=32, activation=\"tanh\")(hidden)\n",
        "\n",
        "    output = keras.layers.Dense(units=1, activation='sigmoid')(hidden)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=model_input, outputs=output, name=model_name)\n",
        "\n",
        "    return model\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "embedding_dim = 100\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = keras.losses.BinaryCrossentropy()\n",
        "\n",
        "model = binary_cnn_with_embeddings(1500,25000, embedding_dim)\n",
        "model.compile(optimizer = optimizer, loss = loss, metrics = 'binary_accuracy')\n",
        "model.summary()\n",
        "model.fit(x = x_train, y = y_trainM.values, validation_data = (x_test, y_testM), epochs = 3, class_weight = {0:.1, 1:.9})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"cnn_with_embeddings\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 1500)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 1500, 100)         2500000   \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 499, 256)          128256    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,671,521\n",
            "Trainable params: 2,671,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "364/364 [==============================] - 48s 130ms/step - loss: 0.0587 - binary_accuracy: 0.8943 - val_loss: 0.1763 - val_binary_accuracy: 0.9355\n",
            "Epoch 2/3\n",
            "364/364 [==============================] - 47s 130ms/step - loss: 0.0445 - binary_accuracy: 0.9319 - val_loss: 0.2004 - val_binary_accuracy: 0.9443\n",
            "Epoch 3/3\n",
            "364/364 [==============================] - 47s 129ms/step - loss: 0.0462 - binary_accuracy: 0.9261 - val_loss: 0.2639 - val_binary_accuracy: 0.9296\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f752d5de6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1QfMyuU3kxb",
        "outputId": "637a925d-5f18-4ec3-9f59-053021ee2afd"
      },
      "source": [
        "confusion_matrix(y_testM, model.predict(x_test)>.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3206,  226],\n",
              "       [  47,  398]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1FdLLN-68vL"
      },
      "source": [
        "Even our simple prototype models are yeilding promising results with the training data, and we believe that our SOTA models, fine tuned with unlabeled training data, will be able to produce even better results.\n",
        "\n",
        "Our experiments in fine-tuning some of these models is outlined in our second notebook. "
      ]
    }
  ]
}